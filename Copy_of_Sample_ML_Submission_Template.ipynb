{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Netflix Movie and TV Shows Clustering**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Learning (Clustering & Content-Based Recommendation System)\n",
        "##### **Contribution**    - Individual\n",
        "**Prepared by**: Kushboo Jain\n",
        "\n",
        "**Goal** : Explore and analyze Netflix’s movies and TV shows dataset using exploratory data analysis (EDA) and unsupervised learning techniques to:\n",
        "\n",
        "Identify trends in content production\n",
        "\n",
        "Analyze Netflix’s focus on movies vs. TV shows\n",
        "\n",
        "Explore country-wise content distribution\n",
        "\n",
        "Cluster similar content based on textual features to support content-based recommendations"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -** Netflix Data Analysis & ML"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Objective\n",
        "\n",
        "Analyze the Netflix dataset to understand content patterns and trends.\n",
        "\n",
        "Apply unsupervised machine learning techniques to cluster Netflix content.\n",
        "\n",
        "Discover hidden structures in Movies and TV Shows based on metadata and text features.\n",
        "\n",
        "Support content-based recommendation systems and strategic insights.\n",
        "\n",
        "2. Data Cleaning & Preprocessing\n",
        "\n",
        "Handled missing values using mode (categorical) & median (numeric).\n",
        "\n",
        "Outliers treated using IQR method.\n",
        "\n",
        "Categorical encoding: Label Encoding (binary), One-Hot Encoding (multi-class).\n",
        "\n",
        "Text preprocessing: lowercasing, punctuation removal, tokenization, POS tagging, lemmatization, TF-IDF vectorization.\n",
        "\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Charts used:\n",
        "\n",
        "Scatter Plot (Duration vs Release Year)\n",
        "\n",
        "Bar Plot (Content Type Distribution)\n",
        "\n",
        "Box Plot (Duration by Type)\n",
        "\n",
        "Correlation Heatmap\n",
        "\n",
        "Pair Plot\n",
        "\n",
        "Word Cloud (Description Keywords)\n",
        "\n",
        "Choropleth Map (Content by Country)\n",
        "\n",
        "Treemap (Content Distribution by Genre)\n",
        "\n",
        "Insights derived:\n",
        "\n",
        "Movies dominate the catalog.\n",
        "\n",
        "TV shows have higher duration variability.\n",
        "\n",
        "Certain genres and regions dominate Netflix content.\n",
        "\n",
        "Historical trends in duration show independent patterns.\n",
        "\n",
        "4. Feature Engineering\n",
        "\n",
        "Created content age (years since release).\n",
        "\n",
        "Log-transformed duration to reduce skewness.\n",
        "\n",
        "Feature selection using VarianceThreshold and unsupervised methods (e.g., PCA for dimensionality reduction).\n",
        "\n",
        "Dimensionality reduction using PCA (95% variance retained).\n",
        "\n",
        "5. Unsupervised ML Modeling\n",
        "\n",
        "Clustering models used:\n",
        "\n",
        "K-Means Clustering (primary method)\n",
        "\n",
        "Optimal number of clusters determined via Elbow Method and Silhouette Score\n",
        "\n",
        "Hierarchical Clustering (dendrogram analysis for content similarity)\n",
        "\n",
        "Optional: DBSCAN for detecting outliers in content patterns\n",
        "\n",
        "Text-based similarity:\n",
        "\n",
        "Cosine similarity on TF-IDF vectors from description and genres\n",
        "\n",
        "Supports content-based recommendations\n",
        "\n",
        "Cluster analysis:\n",
        "\n",
        "Examined cluster composition by type, genre, country, and duration\n",
        "\n",
        "Visualized clusters using PCA or t-SNE 2D projections\n",
        "\n",
        "6. Business Impact\n",
        "\n",
        "Insights help optimize content strategy and licensing decisions.\n",
        "\n",
        "Supports balancing genre diversity and geographic content coverage.\n",
        "\n",
        "Content-based clustering allows Netflix to recommend similar shows/movies automatically.\n",
        "\n",
        "Identifies niche categories or content gaps for future production.\n",
        "\n",
        "7. Future Work\n",
        "\n",
        "Deploy clustering pipeline for real-time content recommendation.\n",
        "\n",
        "Incorporate more textual features from synopsis, reviews, and subtitles.\n",
        "\n",
        "Explore deep learning embeddings (e.g., BERT) for improved content similarity.\n",
        "\n",
        "Combine user behavior data with content features for hybrid recommendation systems."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kushboo10/Netflix_Movie_and_TVShows_Clustering"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rapid expansion of digital streaming platforms, Netflix has accumulated a vast and diverse library of movies and TV shows spanning multiple countries, genres, and formats. As the content library continues to grow, understanding trends, regional availability, and shifts in platform strategy becomes increasingly challenging using raw data alone. Over the past decade, Netflix has notably increased its focus on TV shows and episodic content compared to movies, reflecting evolving audience preferences and strategic priorities.\n",
        "\n",
        "The primary challenge addressed in this project is to analyze and extract meaningful insights from Netflix’s content data to better understand the composition and evolution of its catalog. This includes identifying patterns in content type distribution, country-wise availability, genre dominance, and historical shifts in content focus. Additionally, given the absence of predefined labels for content similarity, unsupervised learning techniques are required to group similar movies and TV shows based on textual and metadata features.\n",
        "\n",
        "The objective of this project is therefore to perform comprehensive exploratory data analysis (EDA) on the Netflix Movies and TV Shows dataset and apply text-based clustering methods to organize content into meaningful groups. These analyses aim to support strategic decision-making in areas such as content acquisition, regional expansion, and catalog diversification. Moreover, the project demonstrates how a content-based recommendation system can be built using unsupervised machine learning techniques to suggest similar movies and TV shows based on cluster similarity."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    url = 'https://drive.google.com/uc?id=1SkhFdYXdsgOL23yraGCeA_dQ8HBgKcEr'\n",
        "    netflix_df = pd.read_csv(url)\n",
        "    print(\"Netflix dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading Netflix dataset:\", e)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(\"=== First 05 rows ===\")\n",
        "display(netflix_df.head(5))\n",
        "\n",
        "# Dataset info: column types, non-null counts\n",
        "print(\"\\n=== Dataset Info ===\")\n",
        "netflix_df.info()\n",
        "\n",
        "# Dataset shape: rows and columns\n",
        "print(\"\\n=== Dataset Shape ===\")\n",
        "print(netflix_df.shape)\n",
        "\n",
        "# Column names\n",
        "print(\"\\n=== Columns ===\")\n",
        "print(netflix_df.columns)\n",
        "\n",
        "# Summary statistics (numeric + categorical)\n",
        "print(\"\\n=== Summary Statistics ===\")\n",
        "display(netflix_df.describe(include='all'))\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n=== Missing Values ===\")\n",
        "display(netflix_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, cols = netflix_df.shape\n",
        "print(f\"Number of rows: {rows}\")\n",
        "print(f\"Number of columns: {cols}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset info: column names, non-null counts, data types\n",
        "print(\"=== Dataset Info ===\")\n",
        "netflix_df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicate rows in the dataset\n",
        "duplicate_count = netflix_df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing/null values per column\n",
        "missing_values = netflix_df.isnull().sum()\n",
        "\n",
        "print(\"=== Missing Values per Column ===\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Set figure size\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "# Draw a heatmap: yellow = missing, blue = present\n",
        "sns.heatmap(netflix_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Size\n",
        "- The dataset contains **7787 rows** and **12 columns**.\n",
        "\n",
        "### Columns\n",
        "The dataset includes information about various Netflix titles, such as:\n",
        "show_id, type (Movie/TV Show), title, director, cast, country, date_added, release_year, rating, duration, listed_in (genres/categories), description.\n",
        "\n",
        "### Missing Values\n",
        "- Several columns have missing values.\n",
        "- Identified using .isnull().sum() and visualized with a heatmap.\n",
        "- Common columns with missing values include: director, cast, country, and occasionally date_added.\n",
        "\n",
        "### Duplicate Records\n",
        "- Duplicate rows can be detected using .duplicated().sum().\n",
        "- Usually, duplicates are minimal but should be removed before analysis.\n",
        "\n",
        "### Data Types\n",
        "- .info() shows the type of each column:\n",
        "  - object → strings like title, cast, description\n",
        "  - int64 → numeric columns like release_year\n",
        "- Understanding data types helps identify which columns need preprocessing.\n",
        "\n",
        "### Visualization\n",
        "- A heatmap of missing values can quickly highlight patterns in null values.\n",
        "- For example, missing directors may appear mostly for TV Shows.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display all columns in the dataset\n",
        "print(\"=== Columns in the Netflix Dataset ===\")\n",
        "print(netflix_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary statistics for the dataset\n",
        "print(\"=== Summary Statistics (Numeric & Categorical Columns) ===\")\n",
        "display(netflix_df.describe(include='all'))\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Column Name    | Description                                                               | Type        |\n",
        "| -------------- | ------------------------------------------------------------------------- | ----------- |\n",
        "| `show_id`      | Unique identifier for each Netflix title                                  | String      |\n",
        "| `type`         | Type of title: \"Movie\" or \"TV Show\"                                       | Categorical |\n",
        "| `title`        | Name of the movie or TV show                                              | String      |\n",
        "| `director`     | Director(s) of the title (may be missing for some TV shows)               | String      |\n",
        "| `cast`         | Main actors/actresses appearing in the title (may be missing)             | String      |\n",
        "| `country`      | Country where the title was produced (may be missing)                     | String      |\n",
        "| `date_added`   | Date the title was added to Netflix                                       | String/Date |\n",
        "| `release_year` | Year the movie or TV show was originally released                         | Integer     |\n",
        "| `rating`       | Content rating (G, PG, PG-13, TV-MA, etc.)                                | Categorical |\n",
        "| `duration`     | Duration of the title: minutes for movies, number of seasons for TV shows | String      |\n",
        "| `listed_in`    | Genres or categories the title belongs to                                 | String      |\n",
        "| `description`  | Short synopsis or description of the title                                | String      |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for each column\n",
        "for col in netflix_df.columns:\n",
        "    unique_vals = netflix_df[col].nunique()\n",
        "    print(f\"Column '{col}' has {unique_vals} unique values.\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Drop duplicate rows ---\n",
        "netflix_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# --- Step 2: Handle missing values ---\n",
        "netflix_df['director'].fillna('Unknown', inplace=True)\n",
        "netflix_df['cast'].fillna('Unknown', inplace=True)\n",
        "netflix_df['country'].fillna('Unknown', inplace=True)\n",
        "netflix_df['date_added'].fillna('Unknown', inplace=True)\n",
        "netflix_df['rating'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# --- Step 3: Clean 'duration' column ---\n",
        "# Convert to numeric: movies → minutes, TV shows → seasons\n",
        "def duration_to_int(x):\n",
        "    if pd.isnull(x):\n",
        "        return np.nan\n",
        "    if 'Season' in x:\n",
        "        return int(x.split()[0])  # number of seasons\n",
        "    elif 'min' in x:\n",
        "        return int(x.split()[0])  # minutes\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "netflix_df['duration_clean'] = netflix_df['duration'].apply(duration_to_int)\n",
        "\n",
        "# --- Step 4: Convert 'release_year' to numeric ---\n",
        "netflix_df['release_year'] = pd.to_numeric(netflix_df['release_year'], errors='coerce')\n",
        "\n",
        "# --- Step 5: Clean categorical columns ---\n",
        "netflix_df['type'] = netflix_df['type'].str.strip()\n",
        "netflix_df['rating'] = netflix_df['rating'].str.strip()\n",
        "netflix_df['listed_in'] = netflix_df['listed_in'].str.strip()\n",
        "\n",
        "# --- Step 6: Convert 'date_added' to datetime ---\n",
        "netflix_df['date_added'] = pd.to_datetime(netflix_df['date_added'], errors='coerce')\n",
        "\n",
        "# --- Step 7: Optional - Drop rows with missing critical info (title/type) ---\n",
        "netflix_df.dropna(subset=['title', 'type'], inplace=True)\n",
        "\n",
        "print(\"Netflix dataset wrangled successfully!\")\n",
        "\n",
        "# --- Step 8: Preview cleaned dataset ---\n",
        "display(netflix_df.head())\n",
        "print(\"\\nMissing values per column after wrangling:\")\n",
        "display(netflix_df.isnull().sum())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning & Wrangling\n",
        "\n",
        "a) Removed Duplicate Rows\n",
        "\n",
        "Ensured all entries are unique to avoid bias in analysis and clustering.\n",
        "\n",
        "Prevents counting the same title multiple times in content patterns or similarity calculations.\n",
        "\n",
        "b) Handled Missing Values\n",
        "\n",
        "Filled missing values in key categorical columns:\n",
        "\n",
        "director → \"Unknown\"\n",
        "\n",
        "cast → \"Unknown\"\n",
        "\n",
        "country → \"Unknown\"\n",
        "\n",
        "date_added → \"Unknown\"\n",
        "\n",
        "rating → \"Unknown\"\n",
        "\n",
        "Retains as much data as possible while avoiding errors in clustering and textual analysis.\n",
        "\n",
        "c) Converted Data Types\n",
        "\n",
        "release_year → numeric (int) for trend analysis\n",
        "\n",
        "date_added → datetime for time-based analysis\n",
        "\n",
        "duration_clean → numeric values for Movies (minutes) and TV Shows (seasons)\n",
        "\n",
        "d) Cleaned Categorical Columns\n",
        "\n",
        "Stripped extra spaces in type, rating, listed_in\n",
        "\n",
        "Prevents duplicate categories such as \"Movie \" vs \"Movie\", which could distort clustering results\n",
        "\n",
        "e) Created New Columns\n",
        "\n",
        "duration_clean → numeric representation of duration to enable clustering and visualization\n",
        "\n",
        "f) Dropped Rows with Missing Critical Info\n",
        "\n",
        "Removed rows missing title or type as these are essential for content-based grouping\n",
        "\n",
        "2. Insights from the Dataset\n",
        "a) Dataset Composition\n",
        "\n",
        "Columns include: show_id, type, title, director, cast, country, date_added, release_year, rating, duration, listed_in, description\n",
        "\n",
        "Mix of categorical, numeric, and textual data suitable for clustering and similarity-based recommendations\n",
        "\n",
        "b) Missing Data Patterns\n",
        "\n",
        "Columns like director, cast, country have missing values, mostly for TV Shows\n",
        "\n",
        "Filling with \"Unknown\" ensures maximum data retention for unsupervised analysis\n",
        "\n",
        "c) Duration Insights\n",
        "\n",
        "Movies and TV Shows have different duration formats:\n",
        "\n",
        "Movies → minutes\n",
        "\n",
        "TV Shows → number of seasons\n",
        "\n",
        "duration_clean enables numeric comparison and supports clustering based on length\n",
        "\n",
        "d) Release Year & Trends\n",
        "\n",
        "release_year allows analysis of trends in content creation over time\n",
        "\n",
        "Combined with date_added, can explore Netflix’s strategy in adding older vs newer content\n",
        "\n",
        "e) Categorical Insights\n",
        "\n",
        "type enables splitting Movies vs TV Shows for separate cluster analysis\n",
        "\n",
        "rating distribution can guide clusters by content suitability\n",
        "\n",
        "listed_in shows genres/categories, a key feature for content similarity and clustering\n",
        "\n",
        "f) Textual Data\n",
        "\n",
        "description and cast columns are ideal for NLP-based feature extraction\n",
        "\n",
        "Supports:\n",
        "\n",
        "Clustering titles by textual similarity\n",
        "\n",
        "Building content-based recommendation systems\n",
        "\n",
        "Identifying hidden patterns in content themes"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Distribution of Titles by Type\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(data=netflix_df, x='type', palette='Set2')\n",
        "plt.title(\"Netflix Titles by Type: Movies vs TV Shows\", fontsize=14)\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart type: countplot (bar chart)\n",
        "\n",
        "Reason:\n",
        "\n",
        "We want to compare the count of Movies vs TV Shows.\n",
        "\n",
        "Bar charts are ideal for categorical variables.\n",
        "\n",
        "Easy to see which type dominates the catalog."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies usually outnumber TV Shows (or vice versa depending on dataset snapshot).\n",
        "\n",
        "Netflix seems to focus more on the type that has higher counts.\n",
        "\n",
        "If TV Shows dominate, it suggests a strategy for retaining subscribers with episodic content."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive impact:\n",
        "\n",
        "Understanding the catalog balance helps Netflix decide on content investments.\n",
        "\n",
        "For example, if movies dominate, investing in exclusive TV shows could increase retention.\n",
        "\n",
        "Negative growth risks:\n",
        "\n",
        "If one type is underrepresented, it may lead to lower engagement for audiences preferring that type.\n",
        "\n",
        "For instance, too few TV Shows might reduce weekly user engagement, as TV Shows keep subscribers coming back regularly."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Distribution of Titles by Rating\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(data=netflix_df, x='rating', order=netflix_df['rating'].value_counts().index, palette='Set3')\n",
        "plt.title(\"Distribution of Netflix Titles by Rating\", fontsize=14)\n",
        "plt.xlabel(\"Content Rating\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart type: countplot (bar chart)\n",
        "\n",
        "Reason:\n",
        "\n",
        "rating is a categorical variable (G, PG, PG-13, TV-MA, etc.).\n",
        "\n",
        "A bar chart is ideal to compare counts across categories.\n",
        "\n",
        "Helps understand the type of audience content is targeted to."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most titles are often rated TV-MA or PG-13, indicating content for mature audiences.\n",
        "\n",
        "Fewer titles may be for G or PG, showing limited content for kids/family.\n",
        "\n",
        "Highlights Netflix’s focus on adult content, which aligns with a global audience."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive impact:\n",
        "\n",
        "Helps content strategy by understanding which ratings dominate.\n",
        "\n",
        "If targeting family content growth, Netflix can increase production of G/PG titles.\n",
        "\n",
        "Aligns marketing campaigns with target audiences.\n",
        "\n",
        "Negative growth risks:\n",
        "\n",
        "If children/family content is underrepresented, Netflix may lose subscriptions from families.\n",
        "\n",
        "Limited family content can reduce engagement for younger viewers, which could impact long-term subscriber growth."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Number of Netflix Titles Added Over the Years\n",
        "# Extract year from 'date_added' column\n",
        "netflix_df['year_added'] = netflix_df['date_added'].dt.year\n",
        "\n",
        "# Count titles per year\n",
        "titles_per_year = netflix_df['year_added'].value_counts().sort_index()\n",
        "\n",
        "# Chart - Line chart for titles added per year\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(x=titles_per_year.index, y=titles_per_year.values, marker='o', color='purple')\n",
        "plt.title(\"Number of Netflix Titles Added Over the Years\", fontsize=14)\n",
        "plt.xlabel(\"Year Added\")\n",
        "plt.ylabel(\"Number of Titles\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart type: Line chart\n",
        "\n",
        "Reason:\n",
        "\n",
        "Line charts are ideal to show trends over time.\n",
        "\n",
        "Helps understand growth in content additions year by year.\n",
        "\n",
        "Useful to spot peaks or declines in content additions."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix shows steady growth in the number of titles added each year.\n",
        "\n",
        "Certain years may have spikes, possibly due to global expansion or original content releases.\n",
        "\n",
        "Years with fewer titles may indicate strategic shifts or production delays."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive impact:\n",
        "\n",
        "Helps Netflix evaluate content growth trends.\n",
        "\n",
        "Identifies years with strong growth, which can correlate with subscriber growth.\n",
        "\n",
        "Supports future content planning and investment decisions.\n",
        "\n",
        "Negative growth risks:\n",
        "\n",
        "Years with declining title additions could indicate slower content acquisition or production.\n",
        "\n",
        "Could lead to lower engagement if new content does not meet subscriber expectations."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#Top 10 Netflix Genres\n",
        "# Split 'listed_in' by comma and explode to get each genre separately\n",
        "genres_series = netflix_df['listed_in'].dropna().str.split(',').explode().str.strip()\n",
        "\n",
        "# Count top 10 genres\n",
        "top_genres = genres_series.value_counts().head(10)\n",
        "\n",
        "# Chart - Horizontal bar chart for top genres\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_genres.values, y=top_genres.index, palette='coolwarm')\n",
        "plt.title(\"Top 10 Netflix Genres\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Genre\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart type: Horizontal bar chart\n",
        "\n",
        "Reason:\n",
        "\n",
        "Horizontal bars are easier to read for categorical variables with long labels (like genre names).\n",
        "\n",
        "Shows top genres by number of titles, highlighting which content types dominate."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common genres are often:\n",
        "\n",
        "Drama, Comedy, International Movies, Action, etc.\n",
        "\n",
        "Indicates Netflix focuses on popular genres to appeal to a broad audience.\n",
        "\n",
        "Niche genres (like Documentary, Stand-Up Comedy) have fewer titles but can target specific audiences."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive impact:\n",
        "\n",
        "Helps Netflix identify popular genres to invest in.\n",
        "\n",
        "Supports marketing campaigns and recommendation strategies based on genre preferences.\n",
        "\n",
        "Guides content acquisition and production decisions.\n",
        "\n",
        "Negative growth risks:\n",
        "\n",
        "Over-representation of a few genres may neglect niche audiences, limiting engagement diversity.\n",
        "\n",
        "If genres like Family or Kids content are underrepresented, it could impact subscriber growth in certain demographics."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#Top 10 Countries by Number of Netflix Titles\n",
        "# Clean 'country' column and handle missing values\n",
        "countries_series = netflix_df['country'].dropna().str.split(',').explode().str.strip()\n",
        "\n",
        "# Count top 10 countries\n",
        "top_countries = countries_series.value_counts().head(10)\n",
        "\n",
        "# Chart - Horizontal bar chart for top countries\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette='magma')\n",
        "plt.title(\"Top 10 Countries by Number of Netflix Titles\", fontsize=14)\n",
        "plt.xlabel(\"Number of Titles\")\n",
        "plt.ylabel(\"Country\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chart type: Horizontal bar chart\n",
        "\n",
        "Reason:\n",
        "\n",
        "Easy to read country names.\n",
        "\n",
        "Shows which countries contribute the most titles.\n",
        "\n",
        "Explodes multiple countries in a single title to account for co-productions."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top countries often include: United States, India, United Kingdom, Canada, etc.\n",
        "\n",
        "Indicates Netflix invests heavily in US and global English-language content, while also expanding local productions in countries like India, South Korea, and Brazil.\n",
        "\n",
        "Helps identify content diversity by geography."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive impact:\n",
        "\n",
        "Guides local content investment in high-performing countries.\n",
        "\n",
        "Helps in regional marketing strategies to attract more subscribers.\n",
        "\n",
        "Supports global expansion strategy by identifying gaps in underrepresented regions.\n",
        "\n",
        "Negative growth risks:\n",
        "\n",
        "If some regions have very few titles, Netflix may lose subscribers due to lack of local content.\n",
        "\n",
        "Over-concentration in a few countries might limit diverse content appeal globally."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Extract numeric duration in minutes\n",
        "import re\n",
        "netflix_df['duration_minutes'] = netflix_df['duration'].apply(lambda x: int(re.search(r'(\\d+)', x).group(1)) if pd.notna(x) and re.search(r'(\\d+)', x) else None)\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(data=netflix_df.dropna(subset=['duration_minutes']), x='duration_minutes', bins=30, kde=True)\n",
        "plt.title(\"Distribution of Movie Durations\", fontsize=14)\n",
        "plt.xlabel(\"Duration (Minutes)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a histogram to visualize the distribution of movie lengths because it effectively shows how durations are spread across the dataset. It helps identify common movie lengths, outliers, or preferences for shorter or longer movies."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By analyzing the histogram, we can observe:\n",
        "\n",
        "The typical duration range for movies on Netflix.\n",
        "If most movies cluster around certain lengths (e.g., 90-120 minutes).\n",
        "Presence of outliers like very short or very long movies.\n",
        "Trends such as whether Netflix predominantly features movies of a particular duration."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Understanding typical movie durations can inform:\n",
        "\n",
        "Content acquisition strategies—focusing on popular length ranges.\n",
        "Content creation or licensing decisions—matching viewer preferences.\n",
        "Tailoring user experience—recommendation algorithms that consider duration preferences.\n",
        "Improving engagement and satisfaction by aligning offerings with audience habits."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Only movies\n",
        "movies_df = netflix_df[netflix_df['type'] == 'Movie']\n",
        "\n",
        "# Split and count genres\n",
        "genres_series = movies_df['listed_in'].dropna().str.split(',').explode().str.strip()\n",
        "top_genres = genres_series.value_counts().head(10)\n",
        "\n",
        "# Pie chart\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(top_genres, labels=top_genres.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('tab20',10))\n",
        "plt.title(\"Distribution of Top 10 Movie Genres on Netflix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart clearly shows proportions of the top movie genres, making it easy to understand which genres dominate Netflix’s movie catalog."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dramas and International Movies occupy the largest share, showing Netflix’s focus on global storytelling.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps content teams decide which genres to produce more of or diversify into less represented genres."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Movies per release year\n",
        "movies_per_year = movies_df['release_year'].value_counts().sort_index()\n",
        "\n",
        "# Bar chart\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=movies_per_year.index, y=movies_per_year.values, palette='viridis')\n",
        "plt.title(\"Number of Movies Released per Year on Netflix\")\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Number of Movies\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is ideal for trend analysis over years, showing changes in content release over time."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix has a larger number of movies from recent decades."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps identify trends in content acquisition and guide future investment in films from specific periods."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "top_movies_count = movies_df['title'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(y=movies_df['title'], order=top_movies_count.index, palette='plasma')\n",
        "plt.title(\"Top 10 Most Common Movies on Netflix\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Movie Title\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count plot makes it easy to see which movies appear most often in the dataset."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies duplicates or extremely popular movies."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful for quality checks and for marketing insights (highlighting popular content)."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.scatterplot(x='release_year', y='duration', data=movies_df, alpha=0.6)\n",
        "plt.title(\"Movie Duration vs Release Year\")\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Duration (minutes)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots are ideal for observing relationships or trends between two numerical variables."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No major trend in duration over the years, but some older movies are longer."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps Netflix understand historical patterns for licensing decisions."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Treemap (Genre-wise Content Distribution)\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# List of possible genre columns\n",
        "possible_genre_cols = ['listed_in', 'genres', 'genre', 'category', 'Category']\n",
        "\n",
        "# Detect genre column\n",
        "genre_col = None\n",
        "for col in possible_genre_cols:\n",
        "    if col in netflix_df.columns:\n",
        "        genre_col = col\n",
        "        break\n",
        "\n",
        "# Prepare dataframe for treemap\n",
        "treemap_df = netflix_df.copy()\n",
        "\n",
        "if genre_col:\n",
        "    # If genre column exists, split and explode\n",
        "    treemap_df[genre_col] = treemap_df[genre_col].astype(str).str.split(', ')\n",
        "    treemap_df = treemap_df.explode(genre_col)\n",
        "    path_cols = ['type', genre_col]\n",
        "else:\n",
        "    # Fallback: only content type\n",
        "    path_cols = ['type']\n",
        "\n",
        "# Plot treemap\n",
        "fig = px.treemap(\n",
        "    treemap_df,\n",
        "    path=path_cols,\n",
        "    title='Netflix Content Distribution'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treemaps are ideal for visualizing hierarchical relationships between content type and genres in a compact layout."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drama and international genres dominate Netflix’s catalog\n",
        "\n",
        "Movies show higher genre diversity than TV Shows\n",
        "\n",
        "Certain genres are strongly associated with one content type"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps Netflix identify overrepresented genres\n",
        "\n",
        "Supports strategic diversification\n",
        "\n",
        "Heavy concentration in few genres may reduce content novelty"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Word Cloud (Image-based chart)\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Create cleaned text column if not exists\n",
        "if 'description_clean' not in netflix_df.columns:\n",
        "    netflix_df['description_clean'] = netflix_df['description'].astype(str).str.lower()\n",
        "    netflix_df['description_clean'] = netflix_df['description_clean'].apply(\n",
        "        lambda x: re.sub(r'[^\\w\\s]', '', x)  # remove punctuation\n",
        "        if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "# Generate Word Cloud\n",
        "text = \" \".join(netflix_df['description_clean'].dropna().astype(str))\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word clouds visually highlight the most frequent themes and keywords present in Netflix content descriptions."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common words like love, family, life, crime, and drama dominate, indicating Netflix’s strong focus on emotional and storytelling-driven content."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps Netflix identify dominant content themes.\n",
        "\n",
        "Overuse of similar themes may lead to content fatigue and reduced user engagement."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 : World Map (Country-wise content count)\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "country_count = netflix_df['country'].value_counts().reset_index()\n",
        "country_count.columns = ['country', 'count']\n",
        "\n",
        "fig = px.choropleth(\n",
        "    country_count,\n",
        "    locations='country',\n",
        "    locationmode='country names',\n",
        "    color='count',\n",
        "    color_continuous_scale='reds',\n",
        "    title='Netflix Content Distribution Across Countries'\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A choropleth world map is ideal for visualizing geographical distribution and regional dominance."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The USA, India, and the UK contribute the highest volume of Netflix content, while many regions remain underrepresented."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supports global expansion and localization strategy.\n",
        "\n",
        "Low content presence in certain regions may limit subscriber growth in those markets."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Select numeric columns\n",
        "numeric_cols = netflix_df.select_dtypes(include=['int64', 'float64'])\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(numeric_cols.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "plt.title(\"Correlation Between Numeric Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmaps give a visual overview of correlations between numeric variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duration and release year may have weak correlations, indicating independent trends.\n",
        "\n",
        "Helps Netflix analysts understand which features can be used for recommendation systems or predictive modeling."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 15 Pair Plot\n",
        "numeric_cols = netflix_df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "sns.pairplot(numeric_cols)\n",
        "plt.suptitle(\"Pair Plot of Numeric Features\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"Reason: Pair plots show relationships between multiple numeric features.\")\n",
        "print(\"Insight: Weak correlation between duration and release year.\")\n",
        "print(\"Business Impact: Indicates independent content planning strategies.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plots allow simultaneous visualization of:\n",
        "\n",
        "Relationships between multiple numeric variables\n",
        "\n",
        "Distribution patterns (diagonal plots)\n",
        "\n",
        "Correlation trends and outliers"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weak correlation between release year and duration\n",
        "\n",
        "Numeric variables show independent behavior\n",
        "\n",
        "Some outliers are visible in duration"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement 1 – Movies vs TV Shows Duration\n",
        "\n",
        "Research Question:\n",
        "I want to explore whether Movies and TV Shows exhibit distinct duration patterns (duration_clean) that could be used as a feature for clustering and grouping similar content.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Movies and TV Shows have similar duration distributions.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Movies and TV Shows have different duration distributions.\n",
        "\n",
        "Testing/Analysis Approach:\n",
        "Compare the distributions of duration_clean for Movies and TV Shows to determine if duration can serve as a meaningful feature for clustering. A non-parametric test (Mann-Whitney U) will be used because the data is skewed (Movies in minutes, TV Shows in seasons).\n",
        "\n",
        "Hypothetical Statement 2 – Duration by Release Year\n",
        "\n",
        "Research Question:\n",
        "I want to explore whether movies released before 2000 and movies released in or after 2000 exhibit distinct duration patterns (duration_clean) that could help group similar content for clustering.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Movies released before 2000 and movies released in or after 2000 have similar duration distributions.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Movies released before 2000 and movies released in or after 2000 have different duration distributions.\n",
        "\n",
        "Testing/Analysis Approach:\n",
        "Compare the distributions of duration_clean across the two release-year groups. Pearson correlation or Mann-Whitney U test can be used depending on the data distribution. This helps evaluate whether release year is a useful numeric feature for clustering.\n",
        "\n",
        "Hypothetical Statement 3 – Content Type vs Rating\n",
        "\n",
        "Research Question:\n",
        "I want to explore whether the distribution of content type (Movie vs TV Show) differs across ratings (G, PG, PG-13, TV-MA, etc.), which could indicate if rating is a meaningful categorical feature for clustering.\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Content type is independent of rating.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Content type varies across ratings.\n",
        "\n",
        "Testing/Analysis Approach:\n",
        "Perform a Chi-Square test on the contingency table of content type vs rating to assess whether there is a significant association. This analysis identifies if rating can be used as a categorical feature for grouping similar content in clustering."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Movies and TV Shows have similar duration distributions.\n",
        "\n",
        "Alternative Hypothesis (H₁): Movies and TV Shows have different duration distributions."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Extract duration_clean for Movies and TV Shows\n",
        "movies = netflix_df[netflix_df['type'] == 'Movie']['duration_clean'].dropna()\n",
        "shows = netflix_df[netflix_df['type'] == 'TV Show']['duration_clean'].dropna()\n",
        "\n",
        "# Mann-Whitney U Test\n",
        "stat, p_val = mannwhitneyu(movies, shows)\n",
        "\n",
        "print(\"U-Statistic:\", stat)\n",
        "print(\"P-Value:\", p_val)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis → Duration distributions differ between Movies and TV Shows.\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis → No significant difference in duration distributions.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: Mann-Whitney U Test\n",
        "\n",
        "Reason: This test is a non-parametric method used to compare two independent groups when the data is not normally distributed.\n",
        "\n",
        "In this case, Movies are measured in minutes and TV Shows in seasons, so the distributions are skewed and have different scales."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of two independent groups: We are comparing duration distributions for Movies vs TV Shows.\n",
        "\n",
        "Non-normal distribution: The data is skewed, violating assumptions of parametric tests like the t-test.\n",
        "\n",
        "Feature relevance for clustering: The goal is unsupervised learning, so we only want to check if duration can meaningfully differentiate content types.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "U-Statistic: 12,958,009.5 → measures the difference between distributions.\n",
        "\n",
        "P-Value: 0.0 → indicates a statistically significant difference.\n",
        "\n",
        "Conclusion: Reject H₀ → Duration distributions of Movies and TV Shows are significantly different and can be used as a feature for clustering/content grouping."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Movies released before 2000 and movies released in or after 2000 have similar duration distributions.\n",
        "\n",
        "Alternative Hypothesis (H₁): Movies released before 2000 and movies released in or after 2000 have different duration distributions."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Filter only movies\n",
        "movies_df = netflix_df[netflix_df['type'] == 'Movie'].copy()\n",
        "\n",
        "# Create two groups based on release year\n",
        "pre_2000 = movies_df[movies_df['release_year'] < 2000]['duration_clean'].dropna()\n",
        "post_2000 = movies_df[movies_df['release_year'] >= 2000]['duration_clean'].dropna()\n",
        "\n",
        "# Mann-Whitney U Test\n",
        "stat, p_val = mannwhitneyu(pre_2000, post_2000)\n",
        "\n",
        "print(\"U-Statistic:\", stat)\n",
        "print(\"P-Value:\", p_val)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis → Duration distributions differ by release year group.\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis → No significant difference in duration distributions.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: Mann-Whitney U Test\n",
        "\n",
        "Reason: This is a non-parametric test for comparing two independent groups when the data is skewed or not normally distributed.\n",
        "\n",
        "In this case, we are comparing movie durations pre-2000 vs post-2000, and the duration data is skewed (not suitable for Pearson correlation)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of two independent groups: Pre-2000 and post-2000 movie durations.\n",
        "\n",
        "Non-normal distribution: Duration data is skewed, violating assumptions of parametric tests like Pearson correlation.\n",
        "\n",
        "Purpose: Determine if release year influences duration patterns — helps decide if release_year can be a useful feature for clustering.\n",
        "\n",
        "Interpretation of results:\n",
        "\n",
        "U-Statistic: 1,255,130.0 → measures the difference in distributions.\n",
        "\n",
        "P-Value: 1.55e-17 → extremely significant.\n",
        "\n",
        "Conclusion: Reject Null Hypothesis → Duration distributions differ by release year group.\n",
        "\n",
        "Implication for unsupervised learning:\n",
        "The difference in distributions indicates that release year can be a meaningful feature for clustering movies based on duration patterns."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Content type is independent of rating.\n",
        "\n",
        "Alternative Hypothesis (H₁): Content type varies across ratings."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create a contingency table of content type vs rating\n",
        "contingency_table = pd.crosstab(netflix_df['type'], netflix_df['rating'])\n",
        "\n",
        "# Perform Chi-Square Test of Independence\n",
        "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-Square Statistic:\", chi2_stat)\n",
        "print(\"P-Value:\", p_val)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"Expected Frequencies:\\n\", expected)\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis → Content type distribution varies across ratings.\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis → Content type distribution does not vary across ratings.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Used: Chi-Square Test of Independence\n",
        "\n",
        "Reason: This is a non-parametric test used to determine if there is a significant association between two categorical variables: content type and rating."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of two categorical variables: type (Movie/TV Show) and rating (G, PG, PG-13, TV-MA, etc.).\n",
        "\n",
        "Non-parametric: No assumptions about numeric distributions are required.\n",
        "\n",
        "Purpose: To see if rating affects content type, helping decide if rating can be a meaningful feature for clustering in a content-based recommendation system.\n",
        "\n",
        "Interpretation of results:\n",
        "\n",
        "Chi-Square Statistic: Measures the deviation between observed and expected counts.\n",
        "\n",
        "P-Value: If < 0.05 → statistically significant association between content type and rating.\n",
        "\n",
        "Conclusion: Reject H₀ → Content type varies across ratings, making rating a potential feature for clustering."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values\n",
        "\n",
        "# Mode imputation for categorical columns\n",
        "categorical_cols = ['director', 'country', 'rating']\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in netflix_df.columns:\n",
        "        netflix_df[col].fillna(netflix_df[col].mode()[0], inplace=True)\n",
        "\n",
        "# Median imputation for numeric duration\n",
        "if 'duration_clean' in netflix_df.columns:\n",
        "    netflix_df['duration_clean'].fillna(netflix_df['duration_clean'].median(), inplace=True)\n",
        "\n",
        "print(\"Missing values handled:\")\n",
        "print(\"- Mode imputation for categorical columns\")\n",
        "print(\"- Median imputation for duration (robust to outliers)\")\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used mode imputation for categorical columns (director, country)\n",
        "\n",
        "Used median imputation for duration\n",
        "\n",
        "Median is robust against outliers"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers using IQR Method\n",
        "\n",
        "Q1 = netflix_df['duration_clean'].quantile(0.25)\n",
        "Q3 = netflix_df['duration_clean'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "netflix_df['duration_clean'] = netflix_df['duration_clean'].clip(lower_bound, upper_bound)\n",
        "\n",
        "print(\"Outliers treated using IQR method.\")\n",
        "print(\"Extreme duration values capped to upper and lower bounds.\")\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used IQR method\n",
        "\n",
        "Extreme duration values capped to upper/lower bounds\n",
        "\n",
        "Prevents skewed ML model learning"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Encoding\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding for binary column\n",
        "le = LabelEncoder()\n",
        "netflix_df['type_encoded'] = le.fit_transform(netflix_df['type'])\n",
        "\n",
        "# One-Hot Encoding for rating and listed_in (genres)\n",
        "netflix_df = pd.get_dummies(\n",
        "    netflix_df,\n",
        "    columns=['rating', 'listed_in'],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "print(\"Categorical encoding completed:\")\n",
        "print(\"- Label Encoding for binary features\")\n",
        "print(\"- One-Hot Encoding for multi-class categorical features\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding for binary features (type)\n",
        "\n",
        "One-Hot Encoding for genres and ratings\n",
        "\n",
        "Prevents ordinal bias"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contractions\n",
        "\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description'].apply(\n",
        "    lambda x: contractions.fix(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"Contractions expanded.\")\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description_clean'].str.lower()\n",
        "\n",
        "print(\"Text converted to lowercase.\")\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "import re\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description_clean'].apply(\n",
        "    lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"Punctuations removed.\")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs and words containing digits\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description_clean'].apply(\n",
        "    lambda x: re.sub(r'http\\S+|www\\S+|\\\\S*\\\\d\\\\S*', '', x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"URLs and digit-containing words removed.\")\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description_clean'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word not in stop_words])\n",
        "    if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"Stopwords removed.\")\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White Spaces\n",
        "\n",
        "netflix_df['description_clean'] = netflix_df['description_clean'].apply(\n",
        "    lambda x: re.sub(r'\\s+', ' ', x).strip() if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"Extra white spaces removed.\")\n",
        "print(netflix_df[['description', 'description_clean']].head())\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text (simple synonym-based rephrasing)\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def rephrase_text(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            new_words.append(synonyms[0].lemmas()[0].name())\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "netflix_df['description_rephrased'] = netflix_df['description_clean'].apply(\n",
        "    lambda x: rephrase_text(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "print(\"Text rephrasing completed using WordNet synonyms.\")\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "\n",
        "# REQUIRED downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "netflix_df['tokens'] = netflix_df['description_rephrased'].apply(\n",
        "    lambda x: word_tokenize(x) if isinstance(x, str) else []\n",
        ")\n",
        "\n",
        "print(\"Tokenization completed successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# REQUIRED downloads (new naming)\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# POS mapping function\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# POS-aware Lemmatization\n",
        "netflix_df['tokens_normalized'] = netflix_df['tokens'].apply(\n",
        "    lambda tokens: [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in nltk.pos_tag(tokens)\n",
        "    ] if isinstance(tokens, list) else []\n",
        ")\n",
        "\n",
        "print(\"POS tagging + lemmatization completed successfully.\")"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***POS-aware Lemmatization***\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Each token (word) in the text is first tagged with its Part of Speech (POS) using nltk.pos_tag.\n",
        "\n",
        "Based on the POS tag (noun, verb, adjective, adverb, etc.), the WordNet lemmatizer reduces the word to its base or root form.\n",
        "\n",
        "Example: \"running\" → \"run\" (verb), \"better\" → \"good\" (adjective)\n",
        "\n",
        "Reason for Using This Technique:\n",
        "\n",
        "Lemmatization preserves meaningful root words while standardizing variations.\n",
        "\n",
        "Using POS-aware lemmatization ensures context-sensitive reduction: verbs, nouns, adjectives, and adverbs are all lemmatized correctly.\n",
        "\n",
        "This improves the quality of textual analysis for NLP tasks such as:\n",
        "\n",
        "Text clustering\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "Outcome:\n",
        "\n",
        "All tokens in the dataset are normalized to their root forms, reducing redundancy and improving consistency for downstream analysis."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "netflix_df['pos_tags'] = netflix_df['tokens_normalized'].apply(\n",
        "    lambda tokens: nltk.pos_tag(tokens) if isinstance(tokens, list) else []\n",
        ")\n",
        "\n",
        "print(\"POS tagging completed.\")"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Vectorization using TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "netflix_df['final_text'] = netflix_df['tokens_normalized'].apply(\n",
        "    lambda tokens: ' '.join(tokens) if isinstance(tokens, list) else ''\n",
        ")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "X_text = tfidf.fit_transform(netflix_df['final_text'])\n",
        "\n",
        "print(\"Vectorization used: TF-IDF\")\n",
        "print(\"Reason: Highlights important words and reduces effect of common words.\")\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***TF-IDF (Term Frequency – Inverse Document Frequency)***\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Converts textual data into numerical feature vectors.\n",
        "\n",
        "Term Frequency (TF): Counts how often a word appears in a document.\n",
        "\n",
        "Inverse Document Frequency (IDF): Reduces the weight of words that appear in many documents (common words) and increases the weight of rare but important words.\n",
        "\n",
        "Each document is represented as a sparse vector of TF-IDF scores.\n",
        "\n",
        "Reason for Using This Technique:\n",
        "\n",
        "Highlights important words in descriptions while reducing the effect of common words like “the”, “and”, etc.\n",
        "\n",
        "Makes text data suitable for machine learning models.\n",
        "\n",
        "Efficient for recommendation systems, clustering, and NLP tasks.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "All Netflix descriptions are transformed into numerical vectors capturing the importance of each token for modeling or analysis.\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Feature Manipulation\n",
        "\n",
        "# Create content age\n",
        "netflix_df['content_age'] = 2025 - netflix_df['release_year']\n",
        "\n",
        "# Select numeric features\n",
        "X_numeric = netflix_df[['duration_clean', 'content_age']]\n",
        "\n",
        "print(\"Feature manipulation completed.\")\n",
        "print(\"Created feature: content_age to reduce correlation with release_year.\")\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Selection\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Combine numeric + text\n",
        "from scipy.sparse import hstack\n",
        "X_combined = hstack([X_text, X_numeric.values])\n",
        "\n",
        "# Variance Threshold\n",
        "vt = VarianceThreshold(threshold=0.01)\n",
        "X_selected = vt.fit_transform(X_combined)\n",
        "\n",
        "# Model-based importance\n",
        "y = netflix_df['type_encoded']\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_selected, y)\n",
        "\n",
        "print(\"Feature selection methods used:\")\n",
        "print(\"- Variance Threshold to remove low-variance features\")\n",
        "print(\"- Random Forest feature importance to reduce overfitting\")\n",
        "print(\"Important features: duration_clean, content_age, key TF-IDF terms\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Variance Threshold:\n",
        "\n",
        " -  Removes features with very low variance, which carry little to no information.\n",
        "\n",
        "- Helps reduce dimensionality and speeds up model training.\n",
        "\n",
        "2. Model-based Selection (Random Forest Feature Importance):\n",
        "\n",
        "- Uses a Random Forest classifier to evaluate feature importance.\n",
        "\n",
        "- Features with higher importance contribute more to the prediction of the target variable (type_encoded).\n",
        "\n",
        "- Helps reduce overfitting by eliminating uninformative features."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numeric Features:\n",
        "\n",
        "- duration_clean → movie/TV show length\n",
        "\n",
        "- content_age → derived feature capturing content freshness\n",
        "\n",
        "\n",
        "Text Features:\n",
        "\n",
        "- Key TF-IDF terms extracted from normalized description text\n",
        "\n",
        "- Represent important content keywords that help distinguish Movies vs TV Shows\n",
        "\n",
        "\n",
        "Reason for Selection:\n",
        "\n",
        "These features were informative for classification and had sufficient variance to contribute meaningfully to model performance.\n",
        "\n",
        "Combining numeric and text features ensures both structural and semantic information is utilized."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. The duration_clean feature has a skewed distribution, with some movies having extremely long durations compared to the majority.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Transformation Applied:\n",
        "\n",
        "Log Transformation using log1p(duration_clean) → creates a new column duration_log.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Reason for Transformation:\n",
        "\n",
        "Reduces skewness and brings the distribution closer to normal.\n",
        "\n",
        "Stabilizes variance, which helps machine learning models perform better and prevents extreme values from dominating the learning process.\n",
        "\n",
        "---\n",
        "\n",
        "Outcome:\n",
        "\n",
        "The duration_log feature is now more suitable for numerical analysis and modeling.\n",
        "\n",
        "Improves model efficiency and accuracy for algorithms sensitive to feature scale or skewed distributions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ZPQx7OynzRLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "netflix_df['duration_log'] = np.log1p(netflix_df['duration_clean'])\n",
        "\n",
        "print(\"Log transformation applied on duration.\")\n",
        "print(\"Reason: Reduces skewness and stabilizes variance.\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(\n",
        "    netflix_df[['duration_log', 'content_age']]\n",
        ")\n",
        "\n",
        "print(\"Scaling method used: StandardScaler\")\n",
        "print(\"Reason: Required for distance-based and gradient-based models.\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***StandardScaler from sklearn.preprocessing***\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Transforms numeric features to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "Applied to features: duration_log and content_age.\n",
        "\n",
        "---\n",
        "\n",
        "Reason for Using StandardScaler:\n",
        "\n",
        "Ensures all numeric features are on the same scale, preventing features with larger values from dominating model training.\n",
        "\n",
        "Necessary for distance-based algorithms (e.g., KNN, clustering) and gradient-based models (e.g., logistic regression, neural networks).\n",
        "\n",
        "Improves model convergence, stability, and performance.\n",
        "\n",
        "---\n",
        "\n",
        "Outcome:\n",
        "\n",
        "Scaled features are now suitable for machine learning algorithms.\n",
        "\n",
        "Helps models learn patterns efficiently without bias due to differing scales.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "19sy2cw5zqX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes.\n",
        "\n",
        "The dataset includes:\n",
        "\n",
        "Numeric features (e.g., duration_log, content_age)\n",
        "\n",
        "High-dimensional text features (TF-IDF vectors with hundreds of terms)\n",
        "\n",
        "---\n",
        "\n",
        "Reason:\n",
        "\n",
        "High-dimensional data can lead to:\n",
        "\n",
        "Curse of dimensionality → model performance may degrade\n",
        "\n",
        "Increased computational cost → slower training and predictions\n",
        "\n",
        "Overfitting → models may learn noise instead of patterns\n",
        "\n",
        "---\n",
        "\n",
        "Dimensionality Reduction Techniques Considered:\n",
        "\n",
        "Principal Component Analysis (PCA):\n",
        "\n",
        "Reduces numeric/text vector dimensions while retaining most of the variance\n",
        "\n",
        "Improves model efficiency and generalization\n",
        "\n",
        "TruncatedSVD (for sparse TF-IDF matrices):\n",
        "\n",
        "Reduces dimensionality of sparse text features without converting them to dense format\n",
        "\n",
        "---\n",
        "\n",
        "Outcome:\n",
        "\n",
        "Dimensionality reduction makes the dataset more manageable and improves machine learning model performance.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction using PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Dimensionality reduction applied: PCA\")\n",
        "print(\"Reason: Reduces feature space while retaining 95% variance.\")\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality Reduction Technique Used:\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "How It Works:\n",
        "\n",
        "PCA transforms the original numeric features into a smaller set of uncorrelated components (principal components).\n",
        "\n",
        "Each component captures as much variance as possible from the original features.\n",
        "\n",
        "n_components=0.95 → retains 95% of the variance while reducing dimensionality.\n",
        "\n",
        "Reason for Using PCA:\n",
        "\n",
        "Reduces the feature space, making the dataset smaller and more manageable.\n",
        "\n",
        "Helps prevent overfitting by removing redundant or less informative features.\n",
        "\n",
        "Improves computational efficiency and model performance.\n",
        "\n",
        "Retains most of the information from the original features.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "The numeric feature space is now reduced, while maintaining most of the original data variance.\n",
        "\n",
        "Ready for machine learning models that benefit from lower-dimensional input."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(\n",
        "    X_pca, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Unsupervised split completed.\")\n",
        "print(\"Split ratio: 80% Train / 20% Test\")\n",
        "print(\"Reason: Optional split for clustering validation or subset analysis.\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting:\n",
        "\n",
        "Ratio: 80% Train / 20% Test\n",
        "\n",
        "Reason: Optional split for clustering validation or subset analysis."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In unsupervised learning, we do not use target labels (like type) to train models.\n",
        "Even if the number of Movies vs TV Shows is unequal, it does not affect unsupervised algorithms like clustering, PCA, or dimensionality reduction.\n",
        "\n",
        "Reason:\n",
        "\n",
        "Unsupervised learning focuses on finding patterns, similarities, or clusters in the data.\n",
        "\n",
        "All samples are used as-is, regardless of class distribution."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For unsupervised learning, we use the full dataset without resampling\n",
        "X_final = X_pca  # PCA-reduced feature matrix\n",
        "\n",
        "print(\"Data ready for unsupervised learning.\")\n",
        "print(\"Shape of feature matrix:\", X_final.shape)\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique: None required\n",
        "\n",
        "Reason:\n",
        "\n",
        "No oversampling or balancing is performed in unsupervised learning because there is no target variable.\n",
        "\n",
        "All data points are included to allow algorithms to detect natural patterns.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "The dataset is ready for clustering, dimensionality reduction, or other unsupervised learning tasks.\n",
        "\n",
        "Preserves the natural distribution of the data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Determine optimal number of clusters using Elbow Method\n",
        "inertia = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_pca)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow curve\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x=K_range, y=inertia, marker='o')\n",
        "plt.title(\"Elbow Method for Optimal K\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Fit KMeans with chosen number of clusters (say k=4)\n",
        "kmeans_final = KMeans(n_clusters=4, random_state=42)\n",
        "cluster_labels = kmeans_final.fit_predict(X_pca)\n",
        "\n",
        "# Calculate unsupervised metrics\n",
        "sil_score = silhouette_score(X_pca, cluster_labels)\n",
        "ch_score = calinski_harabasz_score(X_pca, cluster_labels)\n",
        "db_score = davies_bouldin_score(X_pca, cluster_labels)\n",
        "\n",
        "# Visualize metrics\n",
        "metrics = {'Silhouette': sil_score, 'Calinski-Harabasz': ch_score, 'Davies-Bouldin': db_score}\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "plt.title(\"Unsupervised Evaluation Metrics for KMeans\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for KMeans (number of clusters)\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "best_k = 0\n",
        "best_score = -1\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    score = silhouette_score(X_pca, labels)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "print(\"Optimal number of clusters based on Silhouette Score:\", best_k)\n",
        "print(\"Best Silhouette Score:\", best_score)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Optimization Technique Used:\n",
        "\n",
        "Silhouette Score-Based Selection for KMeans n_clusters\n",
        "\n",
        "Reason:\n",
        "\n",
        "In unsupervised learning, we don’t have labels to evaluate accuracy.\n",
        "\n",
        "Silhouette score measures how well points are clustered: higher score → points are closer to their own cluster and farther from others.\n",
        "\n",
        "Systematically testing different n_clusters allows us to select the optimal number of clusters without labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Best number of clusters found: e.g., k = 4 (based on silhouette score)\n",
        "\n",
        "Best Silhouette Score: e.g., 0.45 (higher than other tested k values)\n",
        "\n",
        "Outcome:\n",
        "\n",
        "Clustering quality improved by choosing the optimal number of clusters.\n",
        "\n",
        "KMeans now groups similar content (Movies/TV Shows) effectively, which can be used for content-based analysis and unsupervised pattern discovery."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation (Unsupervised)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Use PCA-transformed features\n",
        "X_features = X_pca  # Already scaled and reduced numeric features\n",
        "# Determine optimal number of clusters using Silhouette Score\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X_features)\n",
        "    score = silhouette_score(X_features, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot Silhouette Score vs number of clusters\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x=K_range, y=silhouette_scores, marker='o')\n",
        "plt.title(\"Silhouette Score for Optimal Number of Clusters\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n",
        "\n",
        "# Pick optimal k based on highest silhouette score\n",
        "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
        "print(\"Optimal number of clusters:\", optimal_k)\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit KMeans with optimal clusters\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "netflix_df['cluster'] = kmeans_final.fit_predict(X_features)\n",
        "\n",
        "# Analyze cluster distribution\n",
        "print(\"Cluster distribution:\")\n",
        "print(netflix_df['cluster'].value_counts())\n",
        "\n",
        "# Hyperparameter tuning for KMeans (n_clusters) using silhouette score already done\n",
        "# Optional: check inertia for Elbow Method\n",
        "\n",
        "inertia = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow Curve\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x=range(2,11), y=inertia, marker='o')\n",
        "plt.title(\"Elbow Method for KMeans\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Technique: Silhouette Score & Elbow Method\n",
        "\n",
        "Reason:\n",
        "\n",
        "Silhouette Score measures how well-separated clusters are.\n",
        "\n",
        "The Elbow Method checks inertia (compactness of clusters) to choose optimal k.\n",
        "\n",
        "These do not require labeled data, making them suitable for unsupervised learning."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Number of Clusters Found: e.g., k=3 (based on highest silhouette score)\n",
        "\n",
        "Silhouette Score Improved: e.g., 0.42 → indicates well-separated clusters compared to other k values\n",
        "\n",
        "Outcome: Optimal clusters provide meaningful groupings of Netflix content for analysis and recommendations."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics:\n",
        "\n",
        "Silhouette Score:\n",
        "\n",
        "Measures cohesion within clusters and separation between clusters.\n",
        "\n",
        "Higher score → better-defined clusters.\n",
        "\n",
        "Inertia (Elbow Method):\n",
        "\n",
        "Measures total distance of points from cluster centers.\n",
        "\n",
        "Lower inertia → more compact clusters.\n",
        "\n",
        "Business Implications:\n",
        "\n",
        "Well-defined clusters can identify natural groupings:\n",
        "\n",
        "E.g., Movies vs TV Shows, genres, duration, or textual description patterns.\n",
        "\n",
        "Supports personalized recommendations and targeted content strategies.\n",
        "\n",
        "Helps detect gaps in content categories or overrepresented genres.\n",
        "\n",
        "Overall Business Impact:\n",
        "\n",
        "Enables Netflix to cluster content for better user recommendations.\n",
        "\n",
        "Improves content discovery without requiring labeled data.\n",
        "\n",
        "Supports strategic content planning and user engagement."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation (KMeans / Clustering)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Determine optimal number of clusters using Silhouette Score\n",
        "sil_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(X_pca)\n",
        "    sil = silhouette_score(X_pca, cluster_labels)\n",
        "    sil_scores.append(sil)\n",
        "\n",
        "# Plot Silhouette Score\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x=K_range, y=sil_scores, marker='o')\n",
        "plt.title(\"Silhouette Score for Optimal Clusters\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n",
        "\n",
        "# Choose the optimal number of clusters (highest silhouette score)\n",
        "k_opt = K_range[sil_scores.index(max(sil_scores))]\n",
        "print(\"Optimal number of clusters:\", k_opt)\n",
        "\n",
        "# Fit final KMeans with optimal clusters\n",
        "kmeans_final = KMeans(n_clusters=k_opt, random_state=42)\n",
        "cluster_labels_final = kmeans_final.fit_predict(X_pca)\n",
        "\n",
        "# Add cluster labels to dataframe for analysis\n",
        "netflix_df['cluster_model3'] = cluster_labels_final\n",
        "\n",
        "# Cluster distribution\n",
        "print(\"Cluster distribution:\")\n",
        "print(netflix_df['cluster_model3'].value_counts())\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.lineplot(x=K_range, y=sil_scores, marker='o')\n",
        "plt.title(\"Silhouette Score for Optimal Clusters\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Define range of clusters to test\n",
        "K_range = range(2, 11)\n",
        "\n",
        "sil_scores = []\n",
        "ch_scores = []\n",
        "db_scores = []\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "    sil_scores.append(silhouette_score(X_pca, labels))\n",
        "    ch_scores.append(calinski_harabasz_score(X_pca, labels))\n",
        "    db_scores.append(davies_bouldin_score(X_pca, labels))\n",
        "\n",
        "# Determine the optimal k based on Silhouette Score\n",
        "optimal_k = K_range[sil_scores.index(max(sil_scores))]\n",
        "print(\"Optimal number of clusters:\", optimal_k)\n",
        "\n",
        "# Fit final model\n",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "cluster_labels_final = kmeans_final.fit_predict(X_pca)\n",
        "netflix_df['cluster_model3'] = cluster_labels_final\n",
        "print(\"Cluster distribution:\\n\", netflix_df['cluster_model3'].value_counts())\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter: n_clusters (number of clusters)\n",
        "\n",
        "Technique Used: Internal clustering metrics:\n",
        "\n",
        "Silhouette Score → measures how similar an object is to its own cluster vs other clusters (higher is better)\n",
        "\n",
        "Calinski-Harabasz Index → ratio of between-cluster variance to within-cluster variance (higher is better)\n",
        "\n",
        "Davies-Bouldin Index → measures average similarity between clusters (lower is better)\n",
        "\n",
        "Reason for Using These Metrics:\n",
        "\n",
        "No labeled data available → cannot calculate F1, accuracy, etc.\n",
        "\n",
        "Metrics evaluate natural separation of data points for optimal clustering.\n",
        "\n",
        "Helps identify the best k for KMeans or other clustering methods."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determines optimal number of clusters for segmenting Movies vs TV Shows based on features like duration_log, content_age, and TF-IDF text vectors.\n",
        "\n",
        "Provides actionable insights for content grouping and recommendation without using labels."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clusters help identify natural groups of content.\n",
        "\n",
        "Enables targeted recommendations, content strategy planning, and better user experience without needing labeled outcomes."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected ML Model: KMeans Clustering\n",
        "\n",
        "Reason for Selection:\n",
        "\n",
        "It produced the highest Silhouette Score, indicating well-separated and cohesive clusters.\n",
        "\n",
        "Easy to interpret and visualize cluster distributions.\n",
        "\n",
        "Works well with both numeric features (duration_log, content_age) and high-dimensional text features (TF-IDF vectors).\n",
        "\n",
        "Enables actionable insights for content grouping and recommendation without requiring labeled data.\n",
        "\n",
        "Outcome / Business Impact:\n",
        "\n",
        "Movies and TV Shows are grouped into natural clusters based on duration, age, and description.\n",
        "\n",
        "Supports content-based recommendation and targeted user experiences.\n",
        "\n",
        "Helps identify patterns in content without manually labeling data, saving time and improving strategic decisions."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster Centroids (for KMeans):\n",
        "\n",
        "Each cluster has a centroid in feature space.\n",
        "\n",
        "Features with large differences between cluster centroids are more influential in separating clusters.\n",
        "\n",
        "PCA Loadings:\n",
        "\n",
        "Since you applied PCA, you can examine the principal component loadings to see which original features contribute most to the principal components.\n",
        "\n",
        "Features with high absolute values in the top PCs have the most impact on clustering.\n",
        "\n",
        "Silhouette Analysis & Feature Contribution:\n",
        "\n",
        "Compare cluster distributions for each feature. Features showing the most variation across clusters are important."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the KMeans model\n",
        "import joblib\n",
        "\n",
        "joblib.dump(kmeans_final, \"netflix_kmeans_model.pkl\")\n",
        "print(\"Best unsupervised model saved as netflix_kmeans_model.pkl\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained KMeans model\n",
        "import joblib\n",
        "\n",
        "loaded_model = joblib.load(\"netflix_kmeans_model.pkl\")\n",
        "\n",
        "# Predict clusters for unseen data (example: first 5 samples)\n",
        "sample_prediction = loaded_model.predict(X_pca[:5])  # Use PCA-transformed features\n",
        "\n",
        "print(\"Sanity check predictions on unseen data:\")\n",
        "print(sample_prediction)\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix Content Analysis – Unsupervised Learning Project Summary\n",
        "\n",
        "The Netflix dataset was thoroughly cleaned, preprocessed, and transformed, including:\n",
        "\n",
        "Handling missing values and outliers.\n",
        "\n",
        "Encoding categorical features (Label Encoding for binary, One-Hot Encoding for multi-class).\n",
        "\n",
        "Text preprocessing: contractions expansion, lowercasing, punctuation removal, stopword removal, tokenization, POS-aware lemmatization, and TF-IDF vectorization.\n",
        "\n",
        "Numeric transformations: log transformation and scaling for skewed features (e.g., duration).\n",
        "\n",
        "Dimensionality reduction using PCA to reduce high-dimensional TF-IDF vectors and numeric features, retaining 95% variance.\n",
        "\n",
        "Exploratory Data Analysis (EDA) revealed insights on:\n",
        "\n",
        "Content duration patterns between Movies and TV Shows.\n",
        "\n",
        "Release year trends (pre-2000 vs. post-2000 content).\n",
        "\n",
        "Distribution of ratings and genres.\n",
        "\n",
        "Hypothesis Testing (unsupervised exploration focus):\n",
        "\n",
        "Duration distributions differ between Movies and TV Shows.\n",
        "\n",
        "Movies released before 2000 have distinct duration patterns compared to later releases.\n",
        "\n",
        "Content type (Movie vs. TV Show) varies across ratings.\n",
        "\n",
        "These insights informed feature selection and clustering strategies, rather than predictive modeling.\n",
        "\n",
        "Unsupervised Learning – Clustering (KMeans):\n",
        "\n",
        "Optimal number of clusters determined using Silhouette Score.\n",
        "\n",
        "Netflix content grouped into clusters based on numeric (duration, content age) and textual (TF-IDF) features.\n",
        "\n",
        "Cluster analysis revealed natural groupings, helping understand content similarities for recommendations.\n",
        "\n",
        "Feature Engineering & Importance:\n",
        "\n",
        "Combined numeric and textual features, reducing dimensionality via PCA.\n",
        "\n",
        "Important features for clustering included content duration, content age, and key TF-IDF terms.\n",
        "\n",
        "Feature analysis provides insights into what drives content similarity patterns, aiding strategic decisions.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Clustering helps identify content similarity for personalized recommendations, genre-based suggestions, and content strategy.\n",
        "\n",
        "Insights can be used to group new releases or identify gaps in content offerings.\n",
        "\n",
        "Understanding key features that define clusters allows business teams to prioritize influential factors in content strategy.\n",
        "\n",
        "Limitations & Future Scope:\n",
        "\n",
        "Clustering results may vary depending on selected features and number of clusters.\n",
        "\n",
        "Additional clustering methods (e.g., hierarchical clustering, DBSCAN) could improve pattern discovery.\n",
        "\n",
        "Integrating user interaction data could enhance recommendation systems.\n",
        "\n",
        "Outcome:\n",
        "\n",
        "A complete unsupervised ML pipeline from data cleaning to clustering-ready feature sets.\n",
        "\n",
        "Actionable insights for business decisions without relying on labeled data.\n",
        "\n",
        "Prepared dataset for future supervised models if labels become available."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}